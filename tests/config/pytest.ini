[tool:pytest]
# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    unit: Unit tests - fast, isolated tests with mocked dependencies
    integration: Integration tests - test component interactions
    e2e: End-to-end tests - test complete user workflows
    performance: Performance and load tests - may take longer to run
    slow: Tests that take a long time to run
    gpu: Tests that require GPU resources
    external: Tests that require external services (Redis, Database, etc.)
    workflow: Tests related to workflow execution
    auth: Tests related to authentication and authorization
    api: Tests for API endpoints
    database: Tests that interact with the database
    redis: Tests that require Redis
    s3: Tests that require S3/storage
    celery: Tests that require Celery workers
    webhook: Tests for webhook functionality
    monitoring: Tests for monitoring and metrics
    model: Tests related to model management
    file: Tests for file upload/download
    cleanup: Tests that need cleanup after execution

# Test output
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --cov=src
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-branch
    --cov-fail-under=80
    --durations=10
    --maxfail=5

# Minimum version
minversion = 6.0

# Test timeout
timeout = 300

# Warnings
filterwarnings =
    error
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:celery.*
    ignore::UserWarning:aiohttp.*

# Asyncio settings
asyncio_mode = auto

# Log settings
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Test collection
collect_ignore = [
    "setup.py",
    "conftest.py"
]

# Parallel execution
# n = auto  # Uncomment to enable parallel execution with pytest-xdist